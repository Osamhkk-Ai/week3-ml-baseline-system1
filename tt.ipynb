{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "748193e5",
   "metadata": {},
   "source": [
    "---\n",
    "pagetitle: \"W3 D1\"\n",
    "date: 2025-12-28\n",
    "---\n",
    "\n",
    "# Day 1: Data contract + CLI skeleton\n",
    "\n",
    "**Goal:** turn a feature table into a **clear dataset contract** and a runnable **CLI skeleton**.\n",
    "\n",
    "::: {.muted}\n",
    "Bootcamp • SDAIA Academy\n",
    ":::\n",
    "\n",
    "::: {.notes}\n",
    "Say: “By the end of today, everyone can run `ml-baseline --help`, generate sample data, and write a draft model card.”\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Today’s Flow\n",
    "\n",
    "* **Session 1 (60m):** From “model” → “system” (what we’re shipping)\n",
    "* *Asr Prayer (20m)*\n",
    "* **Session 2 (60m):** Dataset contract (X/y, IDs, leakage, schema)\n",
    "* *Maghrib Prayer (20m)*\n",
    "* **Session 3 (60m):** CLI + repo conventions (Typer, paths, logging)\n",
    "* *Isha Prayer (20m)*\n",
    "* **Hands-on (120m):** Build Day 1 deliverables in the Week 3 repo\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of today, you can:\n",
    "\n",
    "* Explain the difference between an ML **model** and an ML **system**\n",
    "* Define **unit of analysis**, **target**, and **ID passthrough** columns\n",
    "* Describe **leakage** and name 3 common leakage patterns\n",
    "* Write a minimal **input schema contract** for batch inference\n",
    "* Run the Week 3 repo via CLI and create a draft **model card**\n",
    "\n",
    "---\n",
    "\n",
    "## Announcements / admin\n",
    "\n",
    "* This week ends with a **GitHub repo submission** (portfolio-ready)\n",
    "* You should push **at least 1 commit per day**\n",
    "* Hidden tests exist: make changes that are **deterministic** and **reproducible**\n",
    "\n",
    "::: callout-note\n",
    "Capstone teams + ideas are finalized by **Week 5 (Jan 15, 2026)**.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## End-state demo (what you’ll run by Friday)\n",
    "\n",
    "**Train**\n",
    "\n",
    "```bash\n",
    "uv run ml-baseline train --target <your_target>\n",
    "```\n",
    "\n",
    "**Predict**\n",
    "\n",
    "```bash\n",
    "uv run ml-baseline predict --run latest --input <file> --output outputs/preds.csv\n",
    "```\n",
    "\n",
    "::: {.muted}\n",
    "Today we only focus on: data contract + CLI basics.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Week 3 project: what “done” looks like\n",
    "\n",
    "By Friday, your repo can:\n",
    "\n",
    "* Read `data/processed/features.<csv|parquet>`\n",
    "* Train a baseline model and save a **run folder** under `models/runs/<run_id>/`\n",
    "* Save artifacts: model, schema, metrics, holdout tables, env snapshot\n",
    "* Batch predict with schema guardrails\n",
    "* Explain results in `reports/model_card.md` + `reports/eval_summary.md`\n",
    "\n",
    "---\n",
    "\n",
    "## Tool stack (minimal, high ROI)\n",
    "\n",
    "**Core (everyone)**\n",
    "\n",
    "* Python, pandas, numpy\n",
    "* scikit-learn, joblib\n",
    "* typer, logging\n",
    "* pytest, ruff\n",
    "\n",
    "**Optional (stretch)**\n",
    "\n",
    "* pyarrow (Parquet), plotly+kaleido (plots)\n",
    "\n",
    "---\n",
    "\n",
    "## Repo tour (what matters today)\n",
    "\n",
    "```text\n",
    "week3-ml-baseline-system/\n",
    "  data/processed/        # feature table lives here\n",
    "  src/ml_baseline/       # real code (CLI + library)\n",
    "  reports/               # model_card.md will be created today\n",
    "  tests/                 # visible checks (more hidden tests exist)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How grading works (Week 3)\n",
    "\n",
    "* **We run your CLI** (help must work, predictable flags)\n",
    "* **We run your tests** (`pytest`) + style (`ruff`)\n",
    "* Hidden tests check:\n",
    "\n",
    "  * deterministic outputs (seeded)\n",
    "  * correct file locations\n",
    "  * helpful errors (fail fast)\n",
    "\n",
    "# Session 1\n",
    "\n",
    "::: {.muted}\n",
    "From “I trained a model” → “I shipped a baseline ML system”\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Session 1 objectives\n",
    "\n",
    "* Define what “ship-ready baseline” means\n",
    "* Write a problem spec: unit of analysis + target + decision\n",
    "* Recognize leakage (cheating) before it happens\n",
    "\n",
    "---\n",
    "\n",
    "## Model vs system\n",
    "\n",
    ":::: {.columns}\n",
    "::: {.column width=\"50%\"}\n",
    "**A model**\n",
    "\n",
    "* a fitted object (weights)\n",
    "* lives in memory\n",
    "* works on the data you already have\n",
    ":::\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "**A system**\n",
    "\n",
    "* a repeatable pipeline (data → model → artifacts)\n",
    "* saves outputs to disk\n",
    "* can be run by someone else\n",
    ":::\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Why we build a baseline system first\n",
    "\n",
    "* You cannot improve what you cannot **measure**\n",
    "* A baseline gives you a **floor** (minimum performance)\n",
    "* A system makes progress **repeatable** (not “it worked once”)\n",
    "\n",
    "::: callout-tip\n",
    "Baselines are boring on purpose. Boring is reliable.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## The Week 3 loop\n",
    "\n",
    "```text\n",
    "Define → Split → Baseline → Train → Evaluate → Save → Predict → Report\n",
    "```\n",
    "\n",
    "::: {.muted}\n",
    "Today: we focus on Define + “dataset contract” + CLI.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Offline-first rule\n",
    "\n",
    "* Your training reads only from `data/processed/…`\n",
    "* No web calls, no external services\n",
    "* Your repo must run on a CPU laptop\n",
    "\n",
    "::: callout-warning\n",
    "If your model needs internet to run, it is not shippable for this bootcamp.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Problem framing: start with the decision\n",
    "\n",
    "A good ML spec answers:\n",
    "\n",
    "* **What decision changes** because of the prediction?\n",
    "* What costs more: **false positives** or **false negatives**?\n",
    "* What constraints exist (CPU, batch vs online, feature availability)?\n",
    "\n",
    "---\n",
    "\n",
    "## Unit of analysis (UoA)\n",
    "\n",
    "**Unit of analysis = one row = one prediction decision.**\n",
    "\n",
    "Examples:\n",
    "\n",
    "* one row per **user**\n",
    "* one row per **transaction**\n",
    "* one row per **day per store**\n",
    "\n",
    "::: callout-note\n",
    "If your unit of analysis is wrong, your evaluation becomes meaningless.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Target column\n",
    "\n",
    "The target is the thing you want to predict.\n",
    "\n",
    "* Classification: label (e.g., `is_fraud`)\n",
    "* Regression: number (e.g., `amount_spent_next_7d`)\n",
    "\n",
    "::: callout-tip\n",
    "Name targets clearly: `is_*`, `has_*`, `*_usd`, `*_days`.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Positive class (binary classification)\n",
    "\n",
    "If your target is 0/1:\n",
    "\n",
    "* Define what **1** means (the “positive” class)\n",
    "* Report the **positive rate** (class balance)\n",
    "\n",
    "::: callout-warning\n",
    "If positives are rare, accuracy can lie to you.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics follow the decision\n",
    "\n",
    "* If false positives are costly → prefer **precision**\n",
    "* If false negatives are costly → prefer **recall**\n",
    "* For rare positives → report **PR-AUC** (later this week)\n",
    "\n",
    "::: {.muted}\n",
    "Today: just choose a reasonable “primary metric” in your model card draft.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Baseline mindset (preview)\n",
    "\n",
    "Before you tune anything:\n",
    "\n",
    "* run a **dummy baseline** (majority / mean)\n",
    "* write the baseline metrics down\n",
    "* only then: train a real model\n",
    "\n",
    "::: callout-note\n",
    "We start baselines tomorrow. Today you set up the *inputs* and *contracts*.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: write the ML spec (6 minutes)\n",
    "\n",
    "Pick *any* dataset idea (or use our sample).\n",
    "\n",
    "Fill in:\n",
    "\n",
    "1. Unit of analysis = _______\n",
    "2. Target column = _______\n",
    "3. Positive class (if binary) = _______\n",
    "4. Decision enabled = _______\n",
    "5. One constraint = _______\n",
    "\n",
    "**Checkpoint:** you can say the spec in one sentence.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution (example ML spec)\n",
    "\n",
    "* Unit of analysis: **one row per user**\n",
    "* Target: `is_high_value` (1 = high value user)\n",
    "* Decision: who gets a retention offer\n",
    "* Constraint: batch inference, CPU-only\n",
    "\n",
    "---\n",
    "\n",
    "## Leakage: “cheating” without noticing\n",
    "\n",
    "Leakage happens when your features include information that would not exist at prediction time.\n",
    "\n",
    "Two common symptoms:\n",
    "\n",
    "* “Amazing” metrics that feel too good\n",
    "* Big train vs holdout gap (later)\n",
    "\n",
    "---\n",
    "\n",
    "## Common leakage patterns (tabular)\n",
    "\n",
    "* **Future-known** features (post-event info)\n",
    "* **Target proxies** (almost the label)\n",
    "* **IDs** (unique identifiers that models memorize)\n",
    "* **Time leakage** (random split on time-ordered data)\n",
    "\n",
    "---\n",
    "\n",
    "## IDs are not features\n",
    "\n",
    "IDs can be useful, but usually only for:\n",
    "\n",
    "* joining data\n",
    "* grouping splits\n",
    "* passing through predictions for joining back\n",
    "\n",
    "::: callout-warning\n",
    "If you let IDs into the model, it can “memorize” instead of “learn”.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: spot the leakage (6 minutes)\n",
    "\n",
    "You have a table with columns:\n",
    "\n",
    "* `user_id`, `country`, `n_orders`, `total_amount`, `refund_flag`, `is_high_value`\n",
    "\n",
    "1. Which is the **ID**?\n",
    "2. Which is the **target**?\n",
    "3. Which column is suspicious for leakage?\n",
    "\n",
    "**Checkpoint:** you can justify your answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: spot the leakage\n",
    "\n",
    "1. ID: `user_id`\n",
    "2. Target: `is_high_value`\n",
    "3. Suspicious: `refund_flag` (often known after the outcome, or a proxy)\n",
    "\n",
    "::: {.muted}\n",
    "Even if it’s not leakage in your data, treat “post-event” fields as suspicious until proven safe.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** Why is “random split” dangerous for time-based prediction?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** it lets future patterns leak into training, inflating metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "Raise your hand when you can:\n",
    "\n",
    "* define **unit of analysis** in 1 sentence\n",
    "* explain **leakage** in 1 sentence\n",
    "* name one ID column from your own dataset idea\n",
    "\n",
    "::: {.notes}\n",
    "Walk around and spot-check 3 students. Ask them to point at the column names in their table.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Session 1 recap\n",
    "\n",
    "* A model is not a system; a system is repeatable + shippable\n",
    "* Start with a clear **spec**: UoA + target + decision\n",
    "* Treat leakage as a first-class risk\n",
    "\n",
    "# Asr break {.splash}\n",
    "\n",
    "## 20 minutes\n",
    "\n",
    "**When you return:** open the Week 3 repo and locate `data/processed/features.*`.\n",
    "\n",
    "# Session 2\n",
    "\n",
    "::: {.muted}\n",
    "Dataset readiness + input schema contract\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Session 2 objectives\n",
    "\n",
    "* Turn a feature table into **X/y** safely\n",
    "* Separate **features vs IDs vs target**\n",
    "* Write a minimal **input schema** for inference\n",
    "* Add 2 “ML readiness” QA gates\n",
    "\n",
    "---\n",
    "\n",
    "## Feature table assumptions (from Week 2)\n",
    "\n",
    "* One row per unit of analysis\n",
    "* Regenerable (idempotent)\n",
    "* Stored under `data/processed/`\n",
    "* Safe to use offline\n",
    "\n",
    "::: callout-tip\n",
    "Week 3 does not start from raw data. It starts from the feature table.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Turning a feature table into X and y\n",
    "\n",
    "* **y** = the target column\n",
    "* **X** = everything else (except IDs)\n",
    "\n",
    "Two rules:\n",
    "\n",
    "* drop rows where target is missing (for training)\n",
    "* never let target appear at inference time\n",
    "\n",
    "---\n",
    "\n",
    "## Example: X/y split (pandas)\n",
    "\n",
    "```python\n",
    "target = \"is_high_value\"\n",
    "\n",
    "y = df[target]\n",
    "X = df.drop(columns=[target])\n",
    "```\n",
    "\n",
    "::: {.muted}\n",
    "We will also drop ID columns from X.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Feature availability at prediction time\n",
    "\n",
    "Ask for each feature:\n",
    "\n",
    "* Do we know this **before** the decision?\n",
    "* Is it stable, or does it change after the event?\n",
    "* Could it be a “target proxy”?\n",
    "\n",
    "::: callout-warning\n",
    "A feature can be perfectly correlated and still be unusable in production.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## IDs: drop vs passthrough\n",
    "\n",
    ":::: {.columns}\n",
    "::: {.column width=\"50%\"}\n",
    "**Drop from features**\n",
    "\n",
    "* `user_id`\n",
    "* `order_id`\n",
    "* `device_id`\n",
    ":::\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "**Keep as passthrough**\n",
    "\n",
    "* so predictions can be joined back\n",
    "* but never required for inference\n",
    ":::\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: classify columns (5 minutes)\n",
    "\n",
    "Columns:\n",
    "\n",
    "* `user_id`, `country`, `n_orders`, `avg_amount`, `is_high_value`\n",
    "\n",
    "1. Target = ?\n",
    "2. ID passthrough = ?\n",
    "3. Required features = ?\n",
    "\n",
    "**Checkpoint:** you have 3 lists.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: classify columns\n",
    "\n",
    "* Target: `is_high_value`\n",
    "* Optional ID passthrough: `user_id`\n",
    "* Required features: `country`, `n_orders`, `avg_amount`\n",
    "\n",
    "---\n",
    "\n",
    "## Missingness: choose a policy\n",
    "\n",
    "Two beginner-friendly options:\n",
    "\n",
    "1. **Impute** (median/mode)\n",
    "2. Add **missingness flags** (feature is missing on purpose)\n",
    "\n",
    "::: callout-note\n",
    "There is no universal “best” choice. The key is to be consistent and document it.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Dtype drift: CSV vs Parquet\n",
    "\n",
    "* CSV can silently turn numbers into strings\n",
    "* Parquet preserves types (more reliable)\n",
    "\n",
    "::: callout-tip\n",
    "This repo supports CSV by default and Parquet as a stretch (optional dependency).\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** What’s the main risk of using CSV-only pipelines?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** dtype drift (numbers become strings), leading to broken features.\n",
    "\n",
    "---\n",
    "\n",
    "## Data contract: what it is\n",
    "\n",
    "A data contract is a promise:\n",
    "\n",
    "* What columns must exist\n",
    "* What types they should have\n",
    "* What columns are forbidden (like the target)\n",
    "\n",
    "::: {.muted}\n",
    "In Week 3, this is an “input schema” saved with every training run.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Input schema: required vs optional vs forbidden\n",
    "\n",
    "* **Required features:** model inputs (must be present)\n",
    "* **Optional IDs:** kept if present (for joining back)\n",
    "* **Forbidden columns:** must not appear at inference input (e.g., target)\n",
    "\n",
    "---\n",
    "\n",
    "## Minimal schema JSON (example)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"required_feature_columns\": [\"country\", \"n_orders\", \"avg_amount\"],\n",
    "  \"optional_id_columns\": [\"user_id\"],\n",
    "  \"forbidden_columns\": [\"is_high_value\"]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: write your schema (6 minutes)\n",
    "\n",
    "Using your own dataset idea (or sample):\n",
    "\n",
    "1. List 3–10 **required feature columns**\n",
    "2. List 0–3 **optional ID columns**\n",
    "3. List **forbidden** columns (at least the target)\n",
    "\n",
    "**Checkpoint:** you have a JSON-shaped draft (doesn’t need perfect dtypes yet).\n",
    "\n",
    "---\n",
    "\n",
    "## Solution (example schema)\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"required_feature_columns\": [\"country\", \"n_orders\", \"avg_amount\", \"total_amount\"],\n",
    "  \"optional_id_columns\": [\"user_id\"],\n",
    "  \"forbidden_columns\": [\"is_high_value\"]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## QA gates: minimum checks before modeling\n",
    "\n",
    "You should be able to say “yes” to:\n",
    "\n",
    "* table is not empty\n",
    "* target exists\n",
    "* no duplicate rows for your key (if you have one)\n",
    "* required columns exist\n",
    "\n",
    "::: callout-warning\n",
    "Fail fast today, so you don’t debug mystery metrics tomorrow.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Code pattern: 2 must-have checks {.smaller}\n",
    "\n",
    "def require_columns(df, cols):\n",
    "missing = [c for c in cols if c not in df.columns]\n",
    "assert not missing, f\"Missing columns: {missing}\"\n",
    "\n",
    "def assert_no_duplicate_key(df, key_cols):\n",
    "dup = df.duplicated(subset=key_cols, keep=False)\n",
    "assert not dup.any(), f\"Duplicate rows for key={key_cols} (n={dup.sum()})\"\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: predict the failure (4 minutes)\n",
    "\n",
    "Scenario:\n",
    "\n",
    "* Your inference input accidentally contains the target column `is_high_value`\n",
    "\n",
    "What should your predict code do?\n",
    "\n",
    "**Checkpoint:** one sentence answer.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: predict the failure\n",
    "\n",
    "It should **fail fast** with a clear message:\n",
    "\n",
    "* “Forbidden columns present in inference input: ['is_high_value']”\n",
    "\n",
    "---\n",
    "\n",
    "## Session 2 recap\n",
    "\n",
    "* X/y split must drop target (and usually IDs) from X\n",
    "* A schema contract prevents “silent” inference bugs\n",
    "* Add small QA gates early (columns + duplicates)\n",
    "\n",
    "# Maghrib break {.splash}\n",
    "\n",
    "## 20 minutes\n",
    "\n",
    "**When you return:** run `ml-baseline --help` and find the `make-sample-data` command.\n",
    "\n",
    "# Session 3\n",
    "\n",
    "::: {.muted}\n",
    "CLI + repo conventions (Typer, paths, logging)\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Session 3 objectives\n",
    "\n",
    "* Explain why we use a CLI (not only notebooks)\n",
    "* Understand Typer commands + options\n",
    "* Know where files go (paths + artifacts)\n",
    "* Follow repo standards: logging, naming, tests\n",
    "\n",
    "---\n",
    "\n",
    "## Why a CLI?\n",
    "\n",
    "A CLI gives you:\n",
    "\n",
    "* **repeatability** (“same command, same result”)\n",
    "* easy grading (we can run it)\n",
    "* easy collaboration (teammates can run it)\n",
    "\n",
    "::: callout-tip\n",
    "If a teammate can’t run it from the README, it’s not shipped.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Our CLI commands (Week 3)\n",
    "\n",
    "* `ml-baseline make-sample-data`\n",
    "* `ml-baseline train`\n",
    "* `ml-baseline predict`\n",
    "* `ml-baseline show-run`\n",
    "\n",
    "::: {.muted}\n",
    "Today: you will run `--help` and `make-sample-data`.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Typer mental model\n",
    "\n",
    "* `app = typer.Typer()`\n",
    "* `@app.command()` makes a subcommand\n",
    "* options become CLI flags (`--target`, `--input`, `--output`)\n",
    "\n",
    "---\n",
    "\n",
    "## Tiny Typer example\n",
    "\n",
    "```python\n",
    "import typer\n",
    "app = typer.Typer()\n",
    "\n",
    "@app.command()\n",
    "def hello(name: str = \"world\"):\n",
    "    typer.echo(f\"hello {name}\")\n",
    "```\n",
    "\n",
    "::: {.muted}\n",
    "Your repo uses the same idea, just with more commands.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: read `--help` (4 minutes)\n",
    "\n",
    "1. Run `ml-baseline --help`\n",
    "2. Find the subcommand list\n",
    "3. Find one flag under `make-sample-data`\n",
    "\n",
    "**Checkpoint:** you can name 1 command and 1 flag.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: what you should see\n",
    "\n",
    "You should see commands like:\n",
    "\n",
    "* `make-sample-data`\n",
    "* `train`\n",
    "* `predict`\n",
    "\n",
    "And options (examples):\n",
    "\n",
    "* `--target`\n",
    "* `--input`\n",
    "* `--output`\n",
    "\n",
    "---\n",
    "\n",
    "## Repo paths: define them once\n",
    "\n",
    "The goal: no hard-coded strings everywhere.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "class Paths:\n",
    "    def __init__(self, root: Path):\n",
    "        self.root = root\n",
    "\n",
    "    @property\n",
    "    def data_processed_dir(self) -> Path:\n",
    "        return self.root / \"data\" / \"processed\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Artifacts vs scratch outputs\n",
    "\n",
    "* `models/runs/<run_id>/...` = versioned run artifacts (important)\n",
    "* `outputs/` = scratch files (safe to delete)\n",
    "\n",
    "::: callout-note\n",
    "Keep your repo clean: don’t commit huge generated artifacts unless asked.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Logging: what to log\n",
    "\n",
    "Log these at minimum:\n",
    "\n",
    "* input paths\n",
    "* output paths\n",
    "* row counts (“loaded 10,000 rows”)\n",
    "* run folder location\n",
    "\n",
    "---\n",
    "\n",
    "## Logging snippet (copy pattern)\n",
    "\n",
    "```python\n",
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(levelname)s %(name)s: %(message)s\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Fail fast (beginner-friendly rule)\n",
    "\n",
    "If something is required:\n",
    "\n",
    "* assert it early\n",
    "* write an error that tells the user what to do next\n",
    "\n",
    "::: callout-warning\n",
    "Avoid “mystery stack traces” that don’t mention which column/path is wrong.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Micro-exercise: improve an error (5 minutes)\n",
    "\n",
    "Bad error:\n",
    "\n",
    "* “KeyError: 'age'”\n",
    "\n",
    "Rewrite it as a helpful message.\n",
    "\n",
    "**Checkpoint:** one improved error message.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution: helpful error\n",
    "\n",
    "Example:\n",
    "\n",
    "* “Missing required feature columns: ['age']\n",
    "  Check your input file and your schema.”\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Check\n",
    "\n",
    "**Question:** What’s the difference between `models/` and `outputs/`?\n",
    "\n",
    ". . .\n",
    "\n",
    "**Answer:** `models/` stores versioned run artifacts; `outputs/` is scratch output.\n",
    "\n",
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "Raise your hand when:\n",
    "\n",
    "* `ml-baseline --help` runs on your machine\n",
    "* you can explain what a CLI command is\n",
    "* you know where the feature table lives\n",
    "\n",
    "---\n",
    "\n",
    "## Session 3 recap\n",
    "\n",
    "* CLI makes work repeatable and gradable\n",
    "* Typer maps Python functions to commands + flags\n",
    "* Follow standards: paths, logging, and predictable outputs\n",
    "\n",
    "# Isha break {.splash}\n",
    "\n",
    "## 20 minutes\n",
    "\n",
    "**When you return:** you will build Day 1 deliverables (CLI + sample data + model card draft).\n",
    "\n",
    "# Hands-on\n",
    "\n",
    "::: {.muted}\n",
    "Build: Day 1 deliverables (data contract + CLI skeleton)\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Hands-on success criteria (today)\n",
    "\n",
    "By the end, you should have:\n",
    "\n",
    "* `uv run ml-baseline --help` works\n",
    "* `uv run ml-baseline make-sample-data` writes `data/processed/features.csv`\n",
    "* `reports/model_card.md` exists with a draft spec (UoA + target + metric)\n",
    "\n",
    "**Stretch (optional):**\n",
    "\n",
    "* If `pyarrow` is installed, also write `data/processed/features.parquet`\n",
    "\n",
    "---\n",
    "\n",
    "## Project layout (today’s touch points)\n",
    "\n",
    "```text\n",
    "src/ml_baseline/cli.py         # commands\n",
    "src/ml_baseline/sample_data.py # sample feature table\n",
    "src/ml_baseline/io.py          # CSV/Parquet helpers\n",
    "data/processed/                # outputs written here\n",
    "reports/model_card.md          # you create this today\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 1 — Setup + sanity check (15 minutes)\n",
    "\n",
    "1. Create the environment\n",
    "2. Run the CLI help\n",
    "3. Run tests once\n",
    "\n",
    "**Checkpoint:** `pytest` passes and `ml-baseline --help` prints commands.\n",
    "\n",
    "---\n",
    "\n",
    "## Hint — common setup issues\n",
    "\n",
    "* Make sure you are in the repo root (where `pyproject.toml` is)\n",
    "* If tests fail: read the **first** error line that mentions your code\n",
    "* If `ml-baseline` is “not found”: run through `uv run …`\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — setup commands\n",
    "\n",
    "**macOS/Linux**\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "uv run ml-baseline --help\n",
    "uv run pytest\n",
    "```\n",
    "\n",
    "**Windows PowerShell**\n",
    "\n",
    "```powershell\n",
    "uv sync\n",
    "uv run ml-baseline --help\n",
    "uv run pytest\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2 — Generate sample data (15 minutes)\n",
    "\n",
    "1. Run `make-sample-data`\n",
    "2. Confirm `data/processed/features.csv` exists\n",
    "3. Print the first 5 rows (CSV)\n",
    "\n",
    "**Checkpoint:** you can see the column names + 5 rows printed.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — generate + inspect (CSV)\n",
    "\n",
    "**macOS/Linux**\n",
    "\n",
    "```bash\n",
    "uv run ml-baseline make-sample-data\n",
    "ls data/processed\n",
    "python -c \"import pandas as pd; print(pd.read_csv('data/processed/features.csv').head())\"\n",
    "```\n",
    "\n",
    "**Windows PowerShell**\n",
    "\n",
    "```powershell\n",
    "uv run ml-baseline make-sample-data\n",
    "ls data/processed\n",
    "python -c \"import pandas as pd; print(pd.read_csv('data/processed/features.csv').head())\"\n",
    "```\n",
    "\n",
    "::: callout-warning\n",
    "If `features.csv` is missing, do **Task 3** (fix the generator).\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Task 3 — Fix `make-sample-data` to always write CSV (25 minutes)\n",
    "\n",
    "Open `src/ml_baseline/sample_data.py` and ensure:\n",
    "\n",
    "* it writes `features.csv` every time\n",
    "* it is deterministic (seeded)\n",
    "* it returns the CSV path\n",
    "\n",
    "**Checkpoint:** running `make-sample-data` creates `data/processed/features.csv`.\n",
    "\n",
    "---\n",
    "\n",
    "## Hint — where to edit\n",
    "\n",
    "* Edit: `make_sample_feature_table(...)`\n",
    "* Use: `write_tabular(df, path)` from `src/ml_baseline/io.py`\n",
    "* Don’t forget: `data/processed/` directory creation\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — write CSV + optional Parquet {.smaller}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from .config import Paths\n",
    "from .io import parquet_supported, write_tabular\n",
    "\n",
    "def make_sample_feature_table(*, root: Path | None = None, n_users: int = 50, seed: int = 42) -> Path:\n",
    "paths = Paths.from_repo_root() if root is None else Paths(root=root)\n",
    "paths.data_processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "```\n",
    "rng = np.random.default_rng(seed)\n",
    "df = pd.DataFrame({\"user_id\": [f\"u{i:03d}\" for i in range(1, n_users + 1)],\n",
    "                   \"country\": rng.choice([\"US\", \"CA\", \"GB\"], size=n_users),\n",
    "                   \"n_orders\": rng.integers(1, 10, size=n_users)})\n",
    "df[\"avg_amount\"] = rng.normal(10, 3, size=n_users).clip(min=1).round(2)\n",
    "df[\"total_amount\"] = (df[\"n_orders\"] * df[\"avg_amount\"]).round(2)\n",
    "df[\"is_high_value\"] = (df[\"total_amount\"] >= 80).astype(int)\n",
    "\n",
    "csv_path = paths.data_processed_dir / \"features.csv\"\n",
    "write_tabular(df, csv_path)\n",
    "if parquet_supported():\n",
    "    write_tabular(df, paths.data_processed_dir / \"features.parquet\")\n",
    "return csv_path\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 4 — Create `reports/model_card.md` + dataset contract (25 minutes)\n",
    "\n",
    "1. Create `reports/model_card.md`\n",
    "2. Paste the template\n",
    "3. Fill at least:\n",
    "\n",
    "   * unit of analysis\n",
    "   * target + positive class (if binary)\n",
    "   * ID passthrough columns\n",
    "   * primary metric (pick one)\n",
    "\n",
    "**Checkpoint:** file exists and has real text (not only placeholders).\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — model card template (part 1) {.smaller}\n",
    "\n",
    "```markdown\n",
    "# Model Card — Week 3 Baseline\n",
    "\n",
    "## Problem\n",
    "- Predict: <target> for <unit of analysis>\n",
    "- Decision enabled: <what action changes?>\n",
    "- Constraints: CPU-only; offline-first; batch inference\n",
    "\n",
    "## Data (contract)\n",
    "- Feature table: data/processed/features.<csv|parquet>\n",
    "- Unit of analysis: <one row per ...>\n",
    "- Target column: <name>, positive class: <...> (if binary)\n",
    "- Optional IDs (passthrough): <list>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — model card template (part 2) {.smaller}\n",
    "\n",
    "```markdown\n",
    "## Splits (draft for now)\n",
    "- Holdout strategy: random stratified (default) / time / group\n",
    "- Leakage risks: <what could leak?>\n",
    "\n",
    "## Metrics (draft for now)\n",
    "- Primary: <metric> (why it matches the decision)\n",
    "- Baseline: dummy model must be reported\n",
    "\n",
    "## Shipping\n",
    "- Artifacts: model + schema + metrics + holdout tables + env snapshot\n",
    "- Known limitations: <where will it fail?>\n",
    "- Monitoring sketch: <what would you watch?>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## What not to commit (quick rule)\n",
    "\n",
    "Do **not** commit:\n",
    "\n",
    "* `outputs/` (scratch)\n",
    "* large generated datasets under `data/processed/` (unless asked)\n",
    "\n",
    "Do commit:\n",
    "\n",
    "* code under `src/`\n",
    "* `reports/model_card.md`\n",
    "\n",
    "---\n",
    "\n",
    "## Task 5 — Git checkpoint (2 minutes)\n",
    "\n",
    "* `git status`\n",
    "* commit message: `day1: data contract + sample data`\n",
    "* push to GitHub\n",
    "\n",
    "**Checkpoint:** your new commit is visible on GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — Git commands\n",
    "\n",
    "```bash\n",
    "git status\n",
    "git add -A\n",
    "git commit -m \"day1: data contract + sample data\"\n",
    "git push\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Task 6 (stretch) — Parquet support (15 minutes)\n",
    "\n",
    "If your machine can install `pyarrow`:\n",
    "\n",
    "1. Install the extra\n",
    "2. Re-run `make-sample-data`\n",
    "3. Confirm `features.parquet` exists **and** `features.csv` still exists\n",
    "\n",
    "**Checkpoint:** both files exist under `data/processed/`.\n",
    "\n",
    "---\n",
    "\n",
    "## Solution — enable Parquet\n",
    "\n",
    "**macOS/Linux**\n",
    "\n",
    "```bash\n",
    "uv sync --extra parquet\n",
    "uv run ml-baseline make-sample-data\n",
    "ls data/processed\n",
    "```\n",
    "\n",
    "**Windows PowerShell**\n",
    "\n",
    "```powershell\n",
    "uv sync --extra parquet\n",
    "uv run ml-baseline make-sample-data\n",
    "ls data/processed\n",
    "```\n",
    "\n",
    "::: callout-warning\n",
    "If Parquet install fails, stay on CSV. Do not block your progress.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "## Debug playbook (when you’re stuck)\n",
    "\n",
    "1. Re-run the command\n",
    "2. Read the **first** error line that mentions your code\n",
    "3. Identify:\n",
    "\n",
    "   * which file?\n",
    "   * which line?\n",
    "   * which missing column/path?\n",
    "4. Fix one small thing\n",
    "5. Re-run tests\n",
    "\n",
    "---\n",
    "\n",
    "## Stretch goals (optional)\n",
    "\n",
    "* Add 1 new feature to the sample feature table (still deterministic)\n",
    "* Add a simple QA check (e.g., duplicates by `user_id`)\n",
    "* Add `reports/model_card.md` “limitations” section with 2 real risks\n",
    "\n",
    "---\n",
    "\n",
    "## Exit Ticket\n",
    "\n",
    "In 1–2 sentences:\n",
    "\n",
    "**What is an “input schema contract”, and why does it matter?**\n",
    "\n",
    "---\n",
    "\n",
    "## What to do after class (Day 1 assignment)\n",
    "\n",
    "**Due:** before tomorrow’s class\n",
    "\n",
    "1. Push your repo to GitHub (public)\n",
    "2. Ensure these commands work:\n",
    "\n",
    "   * `uv run ml-baseline --help`\n",
    "   * `uv run ml-baseline make-sample-data`\n",
    "3. Commit `reports/model_card.md` draft\n",
    "\n",
    "**Deliverable:** GitHub URL with at least 1 Day 1 commit.\n",
    "\n",
    "::: callout-tip\n",
    "Commit small and often. Your future self will thank you.\n",
    ":::\n",
    "\n",
    "# Thank You! {.splash}\n",
    "\n",
    "<div style=\"width: 300px\">{{< brand logo full >}}</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fca0dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
